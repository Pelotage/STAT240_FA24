---
title: "STAT 240: The Mario Kart Case Study: Solutions"
author: "Cameron Jones"
date: "Fall 2024"
output: rmdformats::readthedown
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      error = TRUE, fig.height = 4)

library(tidyverse)
library(readxl)
library(ggridges)

```

# Overview

## Learning Outcomes

* There may be a few new functions and tricks in these lectures, but the purpose of this case study is not to convey new material. The purpose is to **review** the principles we have learned and to **practice** interactively on a real case study.

## Preliminaries

* **You will need to install the `readxl` and `ggridges` packages.**

* Files to download to `COURSE/lecture/week14-extraReview`:
    - `week14-MarioKart.Rmd`
    - `week14-MarioKart.html`
    - `week14-MarioKart-solutions.Rmd`
    - `week14-MarioKart-solutions.html`
    
* Files to download to `COURSE/data`
    - `CombinedMKScores.xlsx`
    - `MKGhostTimes.csv`
    
# Background & Data

* As mentioned in the learning outcomes, the purpose of this case study is not to convey new material. The purpose is to **review** the principles we have learned and **practice** interactively on a real case study.

---

* **Mario Kart 8 Deluxe** is a popular racing video game released for the Nintendo Switch in 2017.

* Players can choose from a number of characters from the Mario franchise and race on many pre-made racetracks.

* See an [example race here](https://www.youtube.com/watch?v=IpDu9A_ulYo).

## The Dataset

* This dataset was collected and created by STAT 240 instructor Cameron Jones.

* Cameron and his friends loved playing Mario Kart and often disagreed on which racetracks were the most enjoyable to play, so Cameron set out to collect some data and investigate.

* Cameron asked each of his friends to **score every track in the game on a scale from 0 to 10**, where 0 represents a track that is no fun at all, and 10 represents a track that is completely and extremely fun.

```{r}
# You need to install and load the readxl package to read this dataset.

scores = read_excel("../../data/CombinedMKScores.xlsx")
head(scores)
```

> Each row of the dataset represents a single person scoring a single track. 

* The `Score` column is a number from 0 to 10 indicating how fun that `Person` thought that `Course` was.

* `Course` is the name of the racetrack. *Even though some courses have similar names such as "Mario Circuit (New)" and "Mario Circuit (DS)", they are in fact different tracks.*

---

Less importantly to the primary aim of finding out which tracks they score as the most enjoyable:

* `Cup` refers to a mechanic within the game that groups 4 tracks together to be played in a row; for example, the `Mushroom` Cup contains the courses Mario Kart Stadium, Water Park, Sweet Sweet Canyon, and Thwomp Ruins.

* `Origin` refers to the fact that there have been many Mario Kart games before Mario Kart 8 Deluxe, and some tracks from previous games carry over to the newer games. For example, the `Origin` value "Wii" refers to a track that was introduced in Mario Kart Wii. 
  - "New" tracks are making their debut in Mario Kart 8 Deluxe.

* `Person` is a letter A through G indicating which of Cameron and his friends is scoring the given `Course`. (Cameron is Person A, friends are B-G.)

# Exploratory Data Analysis

Below are some exploratory data analysis tasks to explore with this dataset.

### Distribution of Scores

> Create a visualization that shows each unique `Person`'s distribution of `Score`s.

Visualization choices to consider:
- Should we use a histogram or a density plot?
- If we do use a histogram, where should the bars start and stop?

- Should we facet by Person or just use different colors within one plot?

```{r}
# A few options... each one has pros and cons!

# Pros: Better reflects the discrete nature of the data, easier to determine each person's distribution.

# Cons: Difficult to decide where boundaries should be, e.g. should 4.5 be include in the bar with 4.0 or 5.0?
# Also difficult to compare features of distributions that aren't adjacent to each other.

ggplot(scores, aes(Score)) +
  geom_histogram(binwidth = 1, boundary = 0.75, color = "black", fill = "gray50") +
  facet_wrap(vars(Person)) +
  scale_x_continuous(breaks = 0:10)
```

```{r}
# Pros: Easier to identify outlier distribution D, easier to see general shape of overall data.

# Cons: Very hard to follow a single distribution, doesn't reflect the discrete nature of the data.

ggplot(scores, aes(Score, fill = Person)) +
  geom_density(alpha = 0.3)
```

```{r}
# Pros: Easier to follow a single distribution.

# Cons: Difficult to see overall trends, compare non-adjacent distributions at the same point.
ggplot(scores, aes(Score)) +
  geom_density() +
  facet_wrap(vars(Person))
```

- Try using `geom_density_ridges()` within the `ggridges` package! (You will need to install it first with `install.packages("ggridges"))`.

```{r}
# You will need to install and load the ggridges package to run this code.
ggplot(scores, aes(Score, Person)) +
  geom_density_ridges()
```

### Course Superlatives

- By average `Score`, what are the 3 highest rated tracks?

```{r}
scores %>% 
  group_by(Course) %>% 
  summarize(averageScore = mean(Score, na.rm = T)) %>% 
  slice_max(averageScore, n = 3)
```

- By standard deviation of `Score`, what are the 3 most controversial tracks?

```{r}
scores %>% 
  group_by(Course) %>% 
  summarize(Controversy = sd(Score, na.rm = T)) %>% 
  slice_max(Controversy, n = 3)
```

- Some friends were less familiar with Mario Kart and decided to not score some courses. How many courses did each `Person` choose not to rate?

```{r}
scores %>% 
  group_by(Person) %>% 
  summarize(skipped = sum(is.na(Score)))
```

- Which single course has the largest difference between Person A's score and Person B's score (in either direction)?

*Hint: We would love to be able to reference the two scores from separate columns, like A - B; but right now the data is not in the right format to do that! How can we reshape the data for that to be possible?*

```{r}
scores %>% 
  pivot_wider(names_from = Person, values_from = Score) %>% 
  slice_max(abs(A-B), n = 1)
```

# Ghost Times

- After that exploratory data analysis, Cameron had another research question of interest.

> Is there a relationship between the length of a racetrack and its average score by Cameron and friends?

- The "length" of a racetrack is difficult to define in the game, since many tracks have multiple possible paths.

- However, there is a mode within the game in which you can race against a pre-determined, built-in **"ghost racer"** for each course, which completes the course in an average amount of time.

* See an [example ghost racer here](https://www.youtube.com/watch?v=eEqnfGbaeh8).

- We will use the time it takes the built-in ghost racer to complete each course as a proxy (an imperfect, but passable substitute) for its length.

- The ghost times are [found online here](https://mario.fandom.com/wiki/List_of_Mario_Kart_8_Deluxe_Staff_Ghosts%27_statistics) and collected & cleaned into `MKGhostTimes.csv`. 

*Note: At the time Cameron collected the scores data above, there were only 80 tracks in the game. Since then, there were 16 tracks added, which are present in the ghosts data below.*

```{r}
ghosts = read_csv("../../data/MKGhostTimes.csv")
```

- This file has a lot of information about each ghost racer, but we really only care about `Course` and `Time`.

```{r}
ghosts %>% 
  select(Course, Time) %>% 
  glimpse()
```

*Note that the `Time` variable was automatically read in as a `<time>` variable; tidyverse's `read_csv` tries to choose intelligent data types like `<time>` and `<date>` for you, whereas base R's `read.csv` would just read time in as a character column.*

```{r}
# notice: read.csv instead of read_csv
read.csv("../../data/MKGhostTimes.csv") %>% 
  select(Course, Time) %>% 
  glimpse()
```

- The time variable is suitable for plotting and modeling as it is; `lm()` and `ggplot()` will understand how to handle it. 

- It will treat it as its numeric value in milliseconds, which we can see with `as.numeric()`.

```{r}
ghosts %>% 
  mutate(Time = as.numeric(Time)) %>% 
  select(Course, Time) %>% 
  glimpse()
```

---

## Joining

> From the `scores` and `ghosts` dataset, create a new dataset with one row per `Course`, and information about its average score and ghost time. Only include courses that have a known average score and ghost time.

```{r}
averageScores = scores %>% 
  group_by(Course) %>% 
  summarize(averageScore = mean(Score, na.rm = T))

# inner_join would also work no matter the order, full would never work, right_join would only work if you reversed order
joined = left_join(averageScores, ghosts, join_by(Course))
```

## Regression Practice

> Use `lm()` to run a hypothesis test to assess if the "length" (ghost time) of each racetrack affects Cameron and friends' average score of that track. Interpret in context.

```{r}
# Remember; response ~ predictor!
model_object = lm(averageScore ~ Time, joined)

summary(model_object)
```

**We fail to find evidence for a relationship between ghost time and average Cameron and friends' score of Mario Kart 8 Deluxe racetracks. (p = 0.15, two-sided linear regression)**

> Create a residual plot and asssess the linear regression assumptions.

```{r}
ggplot(joined, aes(Time, resid(model_object))) +
  geom_point()
```

- We do not see strong evidence for a curved pattern; linearity is satisfied.

- We do not see strong evidence of asymmetry or a tendency away from 0; normal errors around 0 is satisfied.

*Note: Even though there is non-constant variance, the distribution still looks roughly normal at each vertical strip!*

- We do see evidence of "heteroskedasticity", a fanning out pattern - constant variance is **violated**.

> If time, replicate point estimate, test statistic, and p-value by hand.

```{r}
summary(model_object)

x = joined$Time
y = joined$averageScore

# The cor function isn't smart enough for "time" format, so let's give it raw numbers
r = cor(as.numeric(x), y)

beta_hat_1 = r * sd(y) / sd(x)
beta_hat_0 = mean(y) - beta_hat_1 * mean(x)

c(beta_hat_1, beta_hat_0)

se = 0.0001233

test_stat = beta_hat_1 / se

# test statistic positive; take the right tail and double it
p_value = 2*pt(test_stat, df = nrow(joined) - 2, lower.tail = FALSE)

beta_hat_0
beta_hat_1
p_value
```
